{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970ab42f-e0b5-47e2-bca7-e268dd9acaec",
   "metadata": {},
   "source": [
    "# Notkun gervigreindar fyrir teikningu þrívíddarmynda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c4b5e-f8a0-40e4-9a20-fce177517b49",
   "metadata": {},
   "source": [
    "Nathan Holmes-King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1d3ccaa-e673-4dcf-9068-325a8d71f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from einops import repeat\n",
    "from functools import wraps\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywikibot\n",
    "import random\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stl import mesh\n",
    "import time\n",
    "#from timm.models.layers import DropPath\n",
    "import torch\n",
    "from torch import einsum\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch_cluster import fps\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14fa6-f58f-4887-b619-09ded63df20d",
   "metadata": {},
   "source": [
    "## Inngangsorð\n",
    "Við ætlum að þjálfa gervigreindarlíkan til að teikna þrívíddarmyndir.\n",
    "\n",
    "Tilvísanir:\n",
    "- https://arxiv.org/pdf/2301.11445\n",
    "- https://github.com/1zb/3DShape2VecSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5a80b-f925-4de4-a0cb-4ecfbdd55f23",
   "metadata": {},
   "source": [
    "## Gögn\n",
    "Þessi gögn eru STL-skrár frá Wikimedia Commons. Það eru fimm flokkar:\n",
    "- líkamshlutar\n",
    "- byggingar\n",
    "- rúmfræði\n",
    "- geimfarartæki\n",
    "- styttur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a48e65-ff71-4dff-95a0-621dea4d722a",
   "metadata": {},
   "source": [
    "### Sækja gögn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a80470d-2fe8-4a13-8c49-07d5253f0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "flokkar = ['body parts', 'buildings', 'geometric shapes', 'objects in space', 'sculptures']\n",
    "skrar = {}\n",
    "catnum = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadeeb0f-d6b6-4bdf-9fba-2979297056b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body parts\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "buildings\n",
      "0\n",
      "10\n",
      "20\n",
      "geometric shapes\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "objects in space\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "sculptures\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "commons = pywikibot.Site('commons', 'commons')\n",
    "cn = 0\n",
    "for a in flokkar:\n",
    "    print(a)\n",
    "    cat = pywikibot.Category(commons, 'STL files of ' + a)\n",
    "    catnum[a] = cn\n",
    "    cn += 1\n",
    "    n = 0\n",
    "    for p in cat.members(member_type=['file']):\n",
    "        if n % 10 == 0:\n",
    "            print(n)\n",
    "        mynd = pywikibot.FilePage(p)\n",
    "        try:\n",
    "            tempf = open('/Users/002-nathan/Desktop/Envalys/STLdata/' + a + '_' + p.title()[5:], 'r')\n",
    "            tempf.close()\n",
    "        except FileNotFoundError:\n",
    "            mynd.download(filename='/Users/002-nathan/Desktop/Envalys/STLdata/' + a + '_' + p.title()[5:])\n",
    "        try:\n",
    "            skrar[a].append(p.title()[5:])\n",
    "        except KeyError:\n",
    "            skrar[a] = [p.title()[5:]]\n",
    "        n += 1\n",
    "        if n >= 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af781434-8e30-4444-a693-c601f559cbbe",
   "metadata": {},
   "source": [
    "### Setja upp gögn fyrir notkun\n",
    "Við búum til greypingu (\"embedding\") fyrir punktana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75d3ca54-fe85-4887-acee-34c63f178246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body parts\n",
      "2.312717914581299\n",
      "----\n",
      "buildings\n",
      "1.2082140445709229\n",
      "----\n",
      "geometric shapes\n",
      "1.6727559566497803\n",
      "----\n",
      "objects in space\n",
      "exception (False, 'No lines found, impossible to read')\n",
      "2.0670459270477295\n",
      "----\n",
      "sculptures\n",
      "6.107502698898315\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "pc_pre = []\n",
    "queries_pre = []\n",
    "for cat in skrar:\n",
    "    print(cat)\n",
    "    byrjun = time.time()\n",
    "    for fi in skrar[cat]:\n",
    "        # Load data\n",
    "        gogn = mesh.Mesh.from_file('/Users/002-nathan/Desktop/Envalys/STLdata/' + cat + '_' + fi)\n",
    "        inp = []\n",
    "        for i in range(2048):\n",
    "            inp.append(gogn.v0[random.randint(0, len(gogn.v0) - 1)])\n",
    "        pc_pre.append(torch.tensor([inp]).to(device))\n",
    "        inp = []\n",
    "        for i in range(512):\n",
    "            inp.append(gogn.v0[random.randint(0, len(gogn.v0) - 1)])\n",
    "        queries_pre.append(torch.tensor([inp]).to(device))\n",
    "    print(time.time() - byrjun)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2e0e5-6430-4a8a-8abe-2447a02233e9",
   "metadata": {},
   "source": [
    "## Líkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4a3ebf8-01af-4c9f-9f5f-061c48f9916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(p1, p2):\n",
    "    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2 + (p1[2] - p2[2]) ** 2) ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4516d234-cfeb-4596-898e-c5e3280e09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fps(x, batch, ratio):\n",
    "    \"\"\"\n",
    "    Self-written function, based off work done at Tengdu. Ignores batches entirely.\n",
    "    \"\"\"\n",
    "    first = random.randrange(x.shape[0])\n",
    "    hubChoice = [first]\n",
    "    # Find distance between each user and first hub\n",
    "    distFromHub = []\n",
    "    for i in range(x.shape[0]):\n",
    "        distFromHub.append(dist(x[i], x[first]))\n",
    "    # Find distance from every user to every other user\n",
    "    distb = {}\n",
    "    for a in range(x.shape[0]):\n",
    "        distb[a] = {}\n",
    "    for a in range(x.shape[0]):\n",
    "        for b in range(x.shape[0]):\n",
    "            gd = dist(x[a], x[b])\n",
    "            distb[a][b] = gd\n",
    "            distb[b][a] = gd\n",
    "    # Main loop\n",
    "    while len(hubChoice) / x.shape[0] < ratio:\n",
    "        # Calculate weights for each hub\n",
    "        hubWeight = []\n",
    "        for a in range(x.shape[0]):\n",
    "            d_a = distFromHub[a]\n",
    "            if d_a == 0:\n",
    "                continue\n",
    "            w = []\n",
    "            # Distance to other users\n",
    "            for b in range(x.shape[0]):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                d_b = distb[a][b]\n",
    "                if d_b == 0:\n",
    "                    continue\n",
    "                if d_b > d_a:\n",
    "                    w.append(0)\n",
    "                elif d_b < 0.5:\n",
    "                    w.append(math.log(2*d_a)-1)\n",
    "                else:\n",
    "                    w.append(min(max(0, math.log(d_a/d_b)-1),\n",
    "                                 math.log(2*d_a)-1))\n",
    "            wt = sum(w)\n",
    "            hubWeight.append((a, wt))\n",
    "        # Find hub with highest weight\n",
    "        hubWeight.sort(key=lambda x: x[1], reverse=True)\n",
    "        hubChoice.append(hubWeight[0][0])\n",
    "        for a in range(x.shape[0]):\n",
    "            newd = dist(x[a], x[hubWeight[0][0]])\n",
    "            if newd < distFromHub[a]:\n",
    "                distFromHub[a] = newd\n",
    "    return hubChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50523aef-6ef1-4268-a44d-a5b724773d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache = True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, drop_path_rate = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop_path(self.net(x))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64, drop_path_rate = 0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.drop_path(self.to_out(out))\n",
    "\n",
    "\n",
    "class PointEmbed(nn.Module):\n",
    "    def __init__(self, hidden_dim=48, dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % 6 == 0\n",
    "\n",
    "        self.embedding_dim = hidden_dim\n",
    "        e = torch.pow(2, torch.arange(self.embedding_dim // 6)).float() * np.pi\n",
    "        e = torch.stack([\n",
    "            torch.cat([e, torch.zeros(self.embedding_dim // 6),\n",
    "                        torch.zeros(self.embedding_dim // 6)]),\n",
    "            torch.cat([torch.zeros(self.embedding_dim // 6), e,\n",
    "                        torch.zeros(self.embedding_dim // 6)]),\n",
    "            torch.cat([torch.zeros(self.embedding_dim // 6),\n",
    "                        torch.zeros(self.embedding_dim // 6), e]),\n",
    "        ])\n",
    "        self.register_buffer('basis', e)  # 3 x 16\n",
    "\n",
    "        self.mlp = nn.Linear(self.embedding_dim+3, dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def embed(input, basis):\n",
    "        projections = torch.einsum(\n",
    "            'bnd,de->bne', input, basis)\n",
    "        embeddings = torch.cat([projections.sin(), projections.cos()], dim=2)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input: B x N x 3\n",
    "        embed = self.mlp(torch.cat([self.embed(input, self.basis), input], dim=2)) # B x N x C\n",
    "        return embed\n",
    "\n",
    "\n",
    "class DiagonalGaussianDistribution(object):\n",
    "    def __init__(self, mean, logvar, deterministic=False):\n",
    "        self.mean = mean\n",
    "        self.logvar = logvar\n",
    "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
    "        self.deterministic = deterministic\n",
    "        self.std = torch.exp(0.5 * self.logvar)\n",
    "        self.var = torch.exp(self.logvar)\n",
    "        if self.deterministic:\n",
    "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.mean.device)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.mean.device)\n",
    "        return x\n",
    "\n",
    "    def kl(self, other=None):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        else:\n",
    "            if other is None:\n",
    "                return 0.5 * torch.mean(torch.pow(self.mean, 2)\n",
    "                                       + self.var - 1.0 - self.logvar,\n",
    "                                       dim=[1, 2])\n",
    "            else:\n",
    "                return 0.5 * torch.mean(\n",
    "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
    "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
    "                    dim=[1, 2, 3])\n",
    "\n",
    "    def nll(self, sample, dims=[1,2,3]):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        logtwopi = np.log(2.0 * np.pi)\n",
    "        return 0.5 * torch.sum(\n",
    "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
    "            dim=dims)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth=24,\n",
    "        dim=512,\n",
    "        queries_dim=512,\n",
    "        output_dim = 1,\n",
    "        num_inputs = 2048,\n",
    "        num_latents = 512,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        weight_tie_layers = False,\n",
    "        decoder_ff = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_latents = num_latents\n",
    "\n",
    "        self.cross_attend_blocks = nn.ModuleList([\n",
    "            PreNorm(dim, Attention(dim, dim, heads = 1, dim_head = dim), context_dim = dim),\n",
    "            PreNorm(dim, FeedForward(dim))\n",
    "        ])\n",
    "\n",
    "        self.point_embed = PointEmbed(dim=dim)\n",
    "\n",
    "        get_latent_attn = lambda: PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, drop_path_rate=0.1))\n",
    "        get_latent_ff = lambda: PreNorm(dim, FeedForward(dim, drop_path_rate=0.1))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        cache_args = {'_cache': weight_tie_layers}\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_latent_attn(**cache_args),\n",
    "                get_latent_ff(**cache_args)\n",
    "            ]))\n",
    "\n",
    "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, dim, heads = 1, dim_head = dim), context_dim = dim)\n",
    "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
    "\n",
    "        self.to_outputs = nn.Linear(queries_dim, output_dim) if exists(output_dim) else nn.Identity()\n",
    "\n",
    "    def encode(self, pc):\n",
    "        # pc: B x N x 3\n",
    "        B, N, D = pc.shape\n",
    "        assert N == self.num_inputs\n",
    "        \n",
    "        ###### fps\n",
    "        flattened = pc.view(B*N, D)\n",
    "\n",
    "        batch = torch.arange(B).to(pc.device)\n",
    "        batch = torch.repeat_interleave(batch, N)\n",
    "\n",
    "        pos = flattened\n",
    "\n",
    "        ratio = 1.0 * self.num_latents / self.num_inputs\n",
    "\n",
    "        #idx = fps(pos, batch, ratio=ratio)\n",
    "\n",
    "        #sampled_pc = pos[idx]\n",
    "        sampled_pc = pos[:self.num_latents]  # fps() takes too much time to run\n",
    "        sampled_pc = sampled_pc.view(B, -1, 3)\n",
    "        ######\n",
    "\n",
    "        sampled_pc_embeddings = self.point_embed(sampled_pc)\n",
    "\n",
    "        pc_embeddings = self.point_embed(pc)\n",
    "\n",
    "        cross_attn, cross_ff = self.cross_attend_blocks\n",
    "\n",
    "        x = cross_attn(sampled_pc_embeddings, context = pc_embeddings, mask = None) + sampled_pc_embeddings\n",
    "        x = cross_ff(x) + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self, x, queries):\n",
    "\n",
    "        for self_attn, self_ff in self.layers:\n",
    "            x = self_attn(x) + x\n",
    "            x = self_ff(x) + x\n",
    "\n",
    "        # cross attend from decoder queries to latents\n",
    "        queries_embeddings = self.point_embed(queries)\n",
    "        latents = self.decoder_cross_attn(queries_embeddings, context = x)\n",
    "\n",
    "        # optional decoder feedforward\n",
    "        if exists(self.decoder_ff):\n",
    "            latents = latents + self.decoder_ff(latents)\n",
    "        \n",
    "        return self.to_outputs(latents)\n",
    "\n",
    "    def forward(self, pc, queries):\n",
    "        x = self.encode(pc)\n",
    "\n",
    "        o = self.decode(x, queries).squeeze(-1)\n",
    "\n",
    "        return {'logits': o}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3549ce14-2390-4ce4-bbdc-404566737bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': tensor([[-0.1341, -0.1468, -0.1333, -0.1404, -0.1480, -0.1428, -0.1339, -0.1460,\n",
      "         -0.1353, -0.1349, -0.1458, -0.1483, -0.1472, -0.1344, -0.1360, -0.1343,\n",
      "         -0.1484, -0.1458, -0.1457, -0.1464, -0.1455, -0.1460, -0.1355, -0.1374,\n",
      "         -0.1458, -0.1470, -0.1428, -0.1386, -0.1343, -0.1368, -0.1473, -0.1410,\n",
      "         -0.1338, -0.1405, -0.1448, -0.1476, -0.1445, -0.1482, -0.1453, -0.1469,\n",
      "         -0.1386, -0.1368, -0.1284, -0.1338, -0.1376, -0.1479, -0.1392, -0.1432,\n",
      "         -0.1452, -0.1366, -0.1440, -0.1341, -0.1341, -0.1435, -0.1446, -0.1352,\n",
      "         -0.1471, -0.1337, -0.1322, -0.1430, -0.1333, -0.1484, -0.1489, -0.1284,\n",
      "         -0.1462, -0.1395, -0.1442, -0.1394, -0.1340, -0.1455, -0.1467, -0.1442,\n",
      "         -0.1420, -0.1394, -0.1415, -0.1466, -0.1448, -0.1489, -0.1346, -0.1355,\n",
      "         -0.1409, -0.1476, -0.1452, -0.1408, -0.1473, -0.1467, -0.1404, -0.1478,\n",
      "         -0.1395, -0.1406, -0.1470, -0.1416, -0.1351, -0.1440, -0.1373, -0.1335,\n",
      "         -0.1479, -0.1459, -0.1431, -0.1344, -0.1469, -0.1464, -0.1410, -0.1377,\n",
      "         -0.1394, -0.1478, -0.1415, -0.1465, -0.1465, -0.1453, -0.1463, -0.1458,\n",
      "         -0.1436, -0.1357, -0.1365, -0.1396, -0.1448, -0.1460, -0.1427, -0.1438,\n",
      "         -0.1436, -0.1359, -0.1413, -0.1488, -0.1433, -0.1472, -0.1461, -0.1467,\n",
      "         -0.1385, -0.1477, -0.1412, -0.1452, -0.1399, -0.1471, -0.1389, -0.1389,\n",
      "         -0.1420, -0.1428, -0.1351, -0.1475, -0.1438, -0.1420, -0.1353, -0.1350,\n",
      "         -0.1440, -0.1355, -0.1435, -0.1460, -0.1375, -0.1481, -0.1336, -0.1468,\n",
      "         -0.1480, -0.1468, -0.1470, -0.1341, -0.1369, -0.1395, -0.1367, -0.1351,\n",
      "         -0.1339, -0.1467, -0.1458, -0.1479, -0.1350, -0.1339, -0.1432, -0.1475,\n",
      "         -0.1325, -0.1397, -0.1466, -0.1477, -0.1481, -0.1433, -0.1447, -0.1475,\n",
      "         -0.1488, -0.1358, -0.1391, -0.1452, -0.1441, -0.1466, -0.1451, -0.1469,\n",
      "         -0.1486, -0.1356, -0.1447, -0.1453, -0.1444, -0.1430, -0.1356, -0.1395,\n",
      "         -0.1372, -0.1454, -0.1373, -0.1364, -0.1468, -0.1462, -0.1354, -0.1421,\n",
      "         -0.1433, -0.1406, -0.1342, -0.1448, -0.1429, -0.1383, -0.1441, -0.1471,\n",
      "         -0.1468, -0.1417, -0.1445, -0.1454, -0.1480, -0.1334, -0.1462, -0.1467,\n",
      "         -0.1397, -0.1396, -0.1442, -0.1457, -0.1356, -0.1332, -0.1477, -0.1482,\n",
      "         -0.1477, -0.1411, -0.1461, -0.1474, -0.1343, -0.1459, -0.1493, -0.1342,\n",
      "         -0.1361, -0.1402, -0.1444, -0.1468, -0.1484, -0.1436, -0.1459, -0.1336,\n",
      "         -0.1477, -0.1472, -0.1484, -0.1476, -0.1462, -0.1423, -0.1379, -0.1410,\n",
      "         -0.1331, -0.1488, -0.1350, -0.1360, -0.1388, -0.1467, -0.1446, -0.1390,\n",
      "         -0.1353, -0.1444, -0.1408, -0.1445, -0.1456, -0.1424, -0.1484, -0.1464,\n",
      "         -0.1375, -0.1433, -0.1366, -0.1400, -0.1369, -0.1340, -0.1317, -0.1398,\n",
      "         -0.1424, -0.1426, -0.1475, -0.1487, -0.1453, -0.1472, -0.1435, -0.1339,\n",
      "         -0.1464, -0.1451, -0.1446, -0.1337, -0.1399, -0.1402, -0.1457, -0.1462,\n",
      "         -0.1454, -0.1373, -0.1435, -0.1332, -0.1478, -0.1417, -0.1472, -0.1330,\n",
      "         -0.1468, -0.1482, -0.1472, -0.1471, -0.1445, -0.1474, -0.1410, -0.1482,\n",
      "         -0.1443, -0.1479, -0.1315, -0.1319, -0.1457, -0.1423, -0.1352, -0.1479,\n",
      "         -0.1388, -0.1459, -0.1365, -0.1454, -0.1440, -0.1388, -0.1350, -0.1425,\n",
      "         -0.1404, -0.1441, -0.1415, -0.1493, -0.1474, -0.1464, -0.1324, -0.1475,\n",
      "         -0.1433, -0.1476, -0.1481, -0.1477, -0.1450, -0.1334, -0.1482, -0.1450,\n",
      "         -0.1440, -0.1463, -0.1458, -0.1359, -0.1480, -0.1469, -0.1433, -0.1438,\n",
      "         -0.1438, -0.1460, -0.1451, -0.1431, -0.1367, -0.1468, -0.1432, -0.1479,\n",
      "         -0.1469, -0.1388, -0.1337, -0.1486, -0.1359, -0.1400, -0.1451, -0.1466,\n",
      "         -0.1354, -0.1478, -0.1419, -0.1345, -0.1352, -0.1355, -0.1349, -0.1496,\n",
      "         -0.1364, -0.1465, -0.1488, -0.1317, -0.1468, -0.1326, -0.1478, -0.1379,\n",
      "         -0.1347, -0.1478, -0.1456, -0.1474, -0.1437, -0.1477, -0.1543, -0.1419,\n",
      "         -0.1391, -0.1486, -0.1355, -0.1458, -0.1461, -0.1461, -0.1488, -0.1478,\n",
      "         -0.1353, -0.1365, -0.1416, -0.1479, -0.1467, -0.1338, -0.1472, -0.1474,\n",
      "         -0.1434, -0.1471, -0.1479, -0.1391, -0.1339, -0.1484, -0.1417, -0.1370,\n",
      "         -0.1341, -0.1403, -0.1463, -0.1479, -0.1489, -0.1438, -0.1493, -0.1341,\n",
      "         -0.1441, -0.1414, -0.1417, -0.1482, -0.1444, -0.1380, -0.1328, -0.1326,\n",
      "         -0.1471, -0.1467, -0.1464, -0.1391, -0.1469, -0.1444, -0.1394, -0.1471,\n",
      "         -0.1435, -0.1442, -0.1481, -0.1482, -0.1455, -0.1395, -0.1430, -0.1426,\n",
      "         -0.1394, -0.1356, -0.1470, -0.1458, -0.1361, -0.1355, -0.1472, -0.1458,\n",
      "         -0.1357, -0.1441, -0.1357, -0.1416, -0.1463, -0.1348, -0.1447, -0.1390,\n",
      "         -0.1469, -0.1481, -0.1448, -0.1338, -0.1469, -0.1333, -0.1472, -0.1468,\n",
      "         -0.1463, -0.1459, -0.1469, -0.1343, -0.1349, -0.1386, -0.1456, -0.1478,\n",
      "         -0.1335, -0.1483, -0.1373, -0.1333, -0.1432, -0.1447, -0.1355, -0.1349,\n",
      "         -0.1424, -0.1439, -0.1369, -0.1476, -0.1327, -0.1425, -0.1479, -0.1432,\n",
      "         -0.1388, -0.1331, -0.1346, -0.1481, -0.1458, -0.1420, -0.1419, -0.1343,\n",
      "         -0.1345, -0.1458, -0.1459, -0.1333, -0.1367, -0.1364, -0.1462, -0.1475,\n",
      "         -0.1417, -0.1483, -0.1467, -0.1330, -0.1467, -0.1462, -0.1460, -0.1445]],\n",
      "       device='mps:0', grad_fn=<SqueezeBackward1>)}\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder()\n",
    "fw = model(pc_pre[0], queries_pre[0])\n",
    "print(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8de4eb-a1a3-4e9f-af21-34e2b0947425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
