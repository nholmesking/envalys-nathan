{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970ab42f-e0b5-47e2-bca7-e268dd9acaec",
   "metadata": {},
   "source": [
    "# Notkun gervigreindar fyrir teikningu þrívíddarmynda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863c4b5e-f8a0-40e4-9a20-fce177517b49",
   "metadata": {},
   "source": [
    "Nathan Holmes-King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d3ccaa-e673-4dcf-9068-325a8d71f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from einops import repeat\n",
    "from functools import wraps\n",
    "import math\n",
    "import mcubes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywikibot\n",
    "import random\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stl import mesh\n",
    "import time\n",
    "from timm.models.layers import DropPath\n",
    "import torch\n",
    "from torch import einsum\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch_cluster import fps\n",
    "import trimesh\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f14fa6-f58f-4887-b619-09ded63df20d",
   "metadata": {},
   "source": [
    "## Inngangsorð\n",
    "Við ætlum að þjálfa gervigreindarlíkan til að teikna þrívíddarmyndir.\n",
    "\n",
    "Tilvísanir:\n",
    "- https://arxiv.org/pdf/2301.11445\n",
    "- https://github.com/1zb/3DShape2VecSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5a80b-f925-4de4-a0cb-4ecfbdd55f23",
   "metadata": {},
   "source": [
    "## Gögn\n",
    "Þessi gögn eru STL-skrár frá Wikimedia Commons. Það eru fimm flokkar:\n",
    "- líkamshlutar\n",
    "- byggingar\n",
    "- rúmfræði\n",
    "- geimfarartæki\n",
    "- styttur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a48e65-ff71-4dff-95a0-621dea4d722a",
   "metadata": {},
   "source": [
    "### Sækja gögn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a80470d-2fe8-4a13-8c49-07d5253f0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "flokkar = ['body parts', 'buildings', 'geometric shapes', 'objects in space', 'sculptures']\n",
    "skrar = {}\n",
    "catnum = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadeeb0f-d6b6-4bdf-9fba-2979297056b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body parts\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "buildings\n",
      "0\n",
      "10\n",
      "20\n",
      "geometric shapes\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "objects in space\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "sculptures\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "commons = pywikibot.Site('commons', 'commons')\n",
    "cn = 0\n",
    "for a in flokkar:\n",
    "    print(a)\n",
    "    cat = pywikibot.Category(commons, 'STL files of ' + a)\n",
    "    catnum[a] = cn\n",
    "    cn += 1\n",
    "    n = 0\n",
    "    for p in cat.members(member_type=['file']):\n",
    "        if n % 10 == 0:\n",
    "            print(n)\n",
    "        mynd = pywikibot.FilePage(p)\n",
    "        try:\n",
    "            tempf = open('/Users/002-nathan/Desktop/Envalys/STLdata/' + a + '_' + p.title()[5:], 'r')\n",
    "            tempf.close()\n",
    "        except FileNotFoundError:\n",
    "            mynd.download(filename='/Users/002-nathan/Desktop/Envalys/STLdata/' + a + '_' + p.title()[5:])\n",
    "        try:\n",
    "            skrar[a].append(p.title()[5:])\n",
    "        except KeyError:\n",
    "            skrar[a] = [p.title()[5:]]\n",
    "        n += 1\n",
    "        if n >= 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af781434-8e30-4444-a693-c601f559cbbe",
   "metadata": {},
   "source": [
    "### Setja upp gögn fyrir notkun\n",
    "Við búum til greypingu (\"embedding\") fyrir punktana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75d3ca54-fe85-4887-acee-34c63f178246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body parts\n",
      "6.368010997772217\n",
      "----\n",
      "buildings\n",
      "5.713227987289429\n",
      "----\n",
      "geometric shapes\n",
      "1.5320351123809814\n",
      "----\n",
      "objects in space\n",
      "exception (False, 'No lines found, impossible to read')\n",
      "2.0104262828826904\n",
      "----\n",
      "sculptures\n",
      "6.070322036743164\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "pc_pre = []\n",
    "queries_pre = []\n",
    "for cat in skrar:\n",
    "    print(cat)\n",
    "    byrjun = time.time()\n",
    "    for fi in skrar[cat]:\n",
    "        # Load data\n",
    "        gogn = mesh.Mesh.from_file('/Users/002-nathan/Desktop/Envalys/STLdata/' + cat + '_' + fi)\n",
    "        inp = []\n",
    "        for i in range(2048):\n",
    "            inp.append(gogn.v0[random.randint(0, len(gogn.v0) - 1)])\n",
    "        pc_pre.append(torch.tensor([inp]).to(device))\n",
    "        inp = []\n",
    "        for i in range(256):\n",
    "            inp.append(gogn.v0[random.randint(0, len(gogn.v0) - 1)])\n",
    "        queries_pre.append(torch.tensor([inp]).to(device))\n",
    "    print(time.time() - byrjun)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2e0e5-6430-4a8a-8abe-2447a02233e9",
   "metadata": {},
   "source": [
    "## Líkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4a3ebf8-01af-4c9f-9f5f-061c48f9916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(p1, p2):\n",
    "    return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2 + (p1[2] - p2[2]) ** 2) ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4516d234-cfeb-4596-898e-c5e3280e09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fps(x, batch, ratio):\n",
    "    \"\"\"\n",
    "    Self-written function, based off work done at Tengdu. Ignores batches entirely.\n",
    "    \"\"\"\n",
    "    first = random.randrange(x.shape[0])\n",
    "    hubChoice = [first]\n",
    "    # Find distance between each user and first hub\n",
    "    distFromHub = []\n",
    "    for i in range(x.shape[0]):\n",
    "        distFromHub.append(dist(x[i], x[first]))\n",
    "    # Find distance from every user to every other user\n",
    "    distb = {}\n",
    "    for a in range(x.shape[0]):\n",
    "        distb[a] = {}\n",
    "    for a in range(x.shape[0]):\n",
    "        for b in range(x.shape[0]):\n",
    "            gd = dist(x[a], x[b])\n",
    "            distb[a][b] = gd\n",
    "            distb[b][a] = gd\n",
    "    # Main loop\n",
    "    while len(hubChoice) / x.shape[0] < ratio:\n",
    "        # Calculate weights for each hub\n",
    "        hubWeight = []\n",
    "        for a in range(x.shape[0]):\n",
    "            d_a = distFromHub[a]\n",
    "            if d_a == 0:\n",
    "                continue\n",
    "            w = []\n",
    "            # Distance to other users\n",
    "            for b in range(x.shape[0]):\n",
    "                if a == b:\n",
    "                    continue\n",
    "                d_b = distb[a][b]\n",
    "                if d_b == 0:\n",
    "                    continue\n",
    "                if d_b > d_a:\n",
    "                    w.append(0)\n",
    "                elif d_b < 0.5:\n",
    "                    w.append(math.log(2*d_a)-1)\n",
    "                else:\n",
    "                    w.append(min(max(0, math.log(d_a/d_b)-1),\n",
    "                                 math.log(2*d_a)-1))\n",
    "            wt = sum(w)\n",
    "            hubWeight.append((a, wt))\n",
    "        # Find hub with highest weight\n",
    "        hubWeight.sort(key=lambda x: x[1], reverse=True)\n",
    "        hubChoice.append(hubWeight[0][0])\n",
    "        for a in range(x.shape[0]):\n",
    "            newd = dist(x[a], x[hubWeight[0][0]])\n",
    "            if newd < distFromHub[a]:\n",
    "                distFromHub[a] = newd\n",
    "    return hubChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50523aef-6ef1-4268-a44d-a5b724773d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache = True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, drop_path_rate = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop_path(self.net(x))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64, drop_path_rate = 0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.drop_path(self.to_out(out))\n",
    "\n",
    "\n",
    "class PointEmbed(nn.Module):\n",
    "    def __init__(self, hidden_dim=48, dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hidden_dim % 6 == 0\n",
    "\n",
    "        self.embedding_dim = hidden_dim\n",
    "        e = torch.pow(2, torch.arange(self.embedding_dim // 6)).float() * np.pi\n",
    "        e = torch.stack([\n",
    "            torch.cat([e, torch.zeros(self.embedding_dim // 6),\n",
    "                        torch.zeros(self.embedding_dim // 6)]),\n",
    "            torch.cat([torch.zeros(self.embedding_dim // 6), e,\n",
    "                        torch.zeros(self.embedding_dim // 6)]),\n",
    "            torch.cat([torch.zeros(self.embedding_dim // 6),\n",
    "                        torch.zeros(self.embedding_dim // 6), e]),\n",
    "        ])\n",
    "        self.register_buffer('basis', e)  # 3 x 16\n",
    "\n",
    "        self.mlp = nn.Linear(self.embedding_dim+3, dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def embed(input, basis):\n",
    "        projections = torch.einsum(\n",
    "            'bnd,de->bne', input, basis)\n",
    "        embeddings = torch.cat([projections.sin(), projections.cos()], dim=2)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input: B x N x 3\n",
    "        embed = self.mlp(torch.cat([self.embed(input, self.basis), input], dim=2)) # B x N x C\n",
    "        return embed\n",
    "\n",
    "\n",
    "class DiagonalGaussianDistribution(object):\n",
    "    def __init__(self, mean, logvar, deterministic=False):\n",
    "        self.mean = mean\n",
    "        self.logvar = logvar\n",
    "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
    "        self.deterministic = deterministic\n",
    "        self.std = torch.exp(0.5 * self.logvar)\n",
    "        self.var = torch.exp(self.logvar)\n",
    "        if self.deterministic:\n",
    "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.mean.device)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.mean.device)\n",
    "        return x\n",
    "\n",
    "    def kl(self, other=None):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        else:\n",
    "            if other is None:\n",
    "                return 0.5 * torch.mean(torch.pow(self.mean, 2)\n",
    "                                       + self.var - 1.0 - self.logvar,\n",
    "                                       dim=[1, 2])\n",
    "            else:\n",
    "                return 0.5 * torch.mean(\n",
    "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
    "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
    "                    dim=[1, 2, 3])\n",
    "\n",
    "    def nll(self, sample, dims=[1,2,3]):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        logtwopi = np.log(2.0 * np.pi)\n",
    "        return 0.5 * torch.sum(\n",
    "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
    "            dim=dims)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth=24,\n",
    "        dim=512,\n",
    "        queries_dim=512,\n",
    "        output_dim = 1,\n",
    "        num_inputs = 2048,\n",
    "        num_latents = 512,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        weight_tie_layers = False,\n",
    "        decoder_ff = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_latents = num_latents\n",
    "\n",
    "        self.cross_attend_blocks = nn.ModuleList([\n",
    "            PreNorm(dim, Attention(dim, dim, heads = 1, dim_head = dim), context_dim = dim),\n",
    "            PreNorm(dim, FeedForward(dim))\n",
    "        ])\n",
    "\n",
    "        self.point_embed = PointEmbed(dim=dim)\n",
    "\n",
    "        get_latent_attn = lambda: PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, drop_path_rate=0.1))\n",
    "        get_latent_ff = lambda: PreNorm(dim, FeedForward(dim, drop_path_rate=0.1))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        cache_args = {'_cache': weight_tie_layers}\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_latent_attn(**cache_args),\n",
    "                get_latent_ff(**cache_args)\n",
    "            ]))\n",
    "\n",
    "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, dim, heads = 1, dim_head = dim), context_dim = dim)\n",
    "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
    "\n",
    "        self.to_outputs = nn.Linear(queries_dim, output_dim) if exists(output_dim) else nn.Identity()\n",
    "\n",
    "    def encode(self, pc):\n",
    "        # pc: B x N x 3\n",
    "        B, N, D = pc.shape\n",
    "        assert N == self.num_inputs\n",
    "        \n",
    "        ###### fps\n",
    "        flattened = pc.view(B*N, D)\n",
    "\n",
    "        batch = torch.arange(B).to(pc.device)\n",
    "        batch = torch.repeat_interleave(batch, N)\n",
    "\n",
    "        pos = flattened\n",
    "\n",
    "        ratio = 1.0 * self.num_latents / self.num_inputs\n",
    "\n",
    "        #idx = fps(pos, batch, ratio=ratio)\n",
    "\n",
    "        #sampled_pc = pos[idx]\n",
    "        sampled_pc = pos[:self.num_latents]  # fps() takes too much time to run\n",
    "        sampled_pc = sampled_pc.view(B, -1, 3)\n",
    "        ######\n",
    "\n",
    "        sampled_pc_embeddings = self.point_embed(sampled_pc)\n",
    "        print('sampled_pc', sampled_pc.shape, sampled_pc_embeddings.shape)\n",
    "        pc_embeddings = self.point_embed(pc)\n",
    "\n",
    "        cross_attn, cross_ff = self.cross_attend_blocks\n",
    "\n",
    "        x = cross_attn(sampled_pc_embeddings, context = pc_embeddings, mask = None) + sampled_pc_embeddings\n",
    "        x = cross_ff(x) + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self, x, queries):\n",
    "\n",
    "        for self_attn, self_ff in self.layers:\n",
    "            x = self_attn(x) + x\n",
    "            x = self_ff(x) + x\n",
    "\n",
    "        # cross attend from decoder queries to latents\n",
    "        queries_embeddings = self.point_embed(queries)\n",
    "        latents = self.decoder_cross_attn(queries_embeddings, context = x)\n",
    "        print('latents', latents.shape)\n",
    "        # optional decoder feedforward\n",
    "        if exists(self.decoder_ff):\n",
    "            latents = latents + self.decoder_ff(latents)\n",
    "        \n",
    "        return self.to_outputs(latents)\n",
    "\n",
    "    def forward(self, pc, queries):\n",
    "        x = self.encode(pc)\n",
    "        print('x.shape', x.shape)\n",
    "        o = self.decode(x, queries)\n",
    "        print('o.shape', o.shape)\n",
    "        return {'logits': o.squeeze(-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3549ce14-2390-4ce4-bbdc-404566737bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_pc torch.Size([1, 512, 3]) torch.Size([1, 512, 128])\n",
      "x.shape torch.Size([1, 512, 128])\n",
      "latents torch.Size([1, 256, 128])\n",
      "o.shape torch.Size([1, 256, 1])\n",
      "torch.Size([1, 2048, 3]) torch.Size([1, 256, 3]) torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(dim=128, queries_dim=128)\n",
    "fw = model(pc_pre[0], queries_pre[0])\n",
    "print(pc_pre[0].shape, queries_pre[0].shape, fw['logits'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4f8c6-096a-4528-9c77-2bbd8924c494",
   "metadata": {},
   "source": [
    "Þýðing:\n",
    "- ```sampled_pc.shape = (B, num_latents, D)```\n",
    "- ```sampled_pc_embeddings.shape = x.shape = (B, num_latents, dim)```\n",
    "- ```latents.shape = (B, queries.shape[1], queries_dim)```\n",
    "- ```o.shape = (B, queries.shape[1], output_dim)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b8de4eb-a1a3-4e9f-af21-34e2b0947425",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m verts \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m m \u001b[38;5;241m=\u001b[39m trimesh\u001b[38;5;241m.\u001b[39mTrimesh(verts, faces)\n\u001b[0;32m---> 14\u001b[0m m\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_cond_obj/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{:02d}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:05d}\u001b[39;00m\u001b[38;5;124m.obj\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mdm, category_id, i\u001b[38;5;241m*\u001b[39miters\u001b[38;5;241m+\u001b[39mj))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "density=7\n",
    "gap = 2. / density\n",
    "logits = fw['logits'].detach()\n",
    "                    \n",
    "volume = logits.view(density+1, density+1, density+1).permute(1, 0, 2).cpu().numpy()\n",
    "\n",
    "verts, faces = mcubes.marching_cubes(volume, 0)\n",
    "\n",
    "verts *= gap\n",
    "verts -= 1\n",
    "\n",
    "m = trimesh.Trimesh(verts, faces)\n",
    "\n",
    "m.export('class_cond_obj/{}/{:02d}-{:05d}.obj'.format(args.dm, category_id, i*iters+j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79d478-b810-46c6-8166-fcd174d940b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
