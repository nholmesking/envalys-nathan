{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb1f41a-1e42-4d6d-bc9c-f9dcd67e2dfa",
   "metadata": {},
   "source": [
    "# Vegagreining รก gervitunglamyndum\n",
    "Nathan HK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79a1311-7a3d-44c3-8912-8d6052611252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/k7/ssgp6zcs72j0lsw27nm39kvw0000gn/T/ipykernel_15669/2669378565.py\", line 10, in <module>\n",
      "    from pyrosm import OSM\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/__init__.py\", line 1, in <module>\n",
      "    from pyrosm.data import get_data, get_path  # drop get_path in the future\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/data/__init__.py\", line 2, in <module>\n",
      "    from pyrosm.utils.download import download\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/utils/__init__.py\", line 8, in <module>\n",
      "    import geopandas as gpd\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/__init__.py\", line 3, in <module>\n",
      "    from geopandas.geoseries import GeoSeries\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geoseries.py\", line 9, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/k7/ssgp6zcs72j0lsw27nm39kvw0000gn/T/ipykernel_15669/2669378565.py\", line 10, in <module>\n",
      "    from pyrosm import OSM\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/__init__.py\", line 1, in <module>\n",
      "    from pyrosm.data import get_data, get_path  # drop get_path in the future\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/data/__init__.py\", line 2, in <module>\n",
      "    from pyrosm.utils.download import download\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/utils/__init__.py\", line 8, in <module>\n",
      "    import geopandas as gpd\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/__init__.py\", line 3, in <module>\n",
      "    from geopandas.geoseries import GeoSeries\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/geopandas/geoseries.py\", line 9, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from io import BytesIO\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import psutil\n",
    "from pyrosm import OSM\n",
    "from pyrosm import get_data\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from shapely.geometry import Point\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import tracemalloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73776003-4532-416f-8943-3f9ea869de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappa = '/Users/002-nathan/Desktop/Envalys/gtm/'\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35614a2d-fc85-4085-b036-144f8ce4cbe7",
   "metadata": {},
   "source": [
    "## Inngangsorรฐ\n",
    "Viรฐ viljum รพjรกlfa gervigreindarlรญkan til aรฐ greina vegi รก gervitunglamyndum. Viรฐ รพurfum tvenn gรถgn: gervitunglamyndir og staรฐsetningar vega.\n",
    "- **Gervitunglamyndir:** Viรฐ notum skjรกmyndatรถkur af Jรก.is. รg veit ekki hvort รพetta sรฉ lรถglegt, en รฉg er ekki meรฐ neinar betri leiรฐir.\n",
    "- **Staรฐsetningar vega:** Viรฐ notum OpenStreetMap.\n",
    "\n",
    "รg nota Apple M1 Pro-รถrgjรถrvi meรฐ 16 GB minni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ca15a-f705-45bc-a0a4-8c96bcfded27",
   "metadata": {},
   "source": [
    "## Gervitunglamyndir\n",
    "Viรฐ notum lista yfir greinar รก ensku Wikipediunni um staรฐi รก Hรถfuรฐborgarsvรฆรฐinu og Akureyri. Gรถgnin รก Landsbyggรฐinni eru ekki nรณg nรกkvรฆm fyrir รพetta lรญkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611f2f21-144c-4ed2-abb1-c98e8e015757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "byrjun = time.time()\n",
    "hnitlisti = []\n",
    "hnit_sv = {'h':[(64.167, 64.073, -21.649992, -22.041892, 30, 40),\n",
    "                (64.073, 64.033177, -21.871, -22.041892, 20, 20),\n",
    "                (64.200015, 64.167, -21.649992, -21.763, 10, 10)],\n",
    "           'a':[(65.706074, 65.656070, -18.073935, -18.148693, 10, 10)]}\n",
    "for svk in hnit_sv:\n",
    "    for hnit_h in hnit_sv[svk]:\n",
    "        diff = (hnit_h[0] - hnit_h[1], hnit_h[2] - hnit_h[3])\n",
    "        for i in range(hnit_h[4]):\n",
    "            lat = round(hnit_h[1] + (i * 2 + 1) * diff[0] / 60, 6)\n",
    "            for j in range(hnit_h[5]):\n",
    "                lon = round(hnit_h[3] + (j * 2 + 1) * diff[1] / 60, 6)\n",
    "                hnitlisti.append((svk, lat, lon))\n",
    "\"\"\"\n",
    "for a in os.listdir(mappa):\n",
    "    s = a.split('_')\n",
    "    if len(s) != 3 or s[0] not in ['h', 'a', 'd'] or s[2][-4:] != '.png':\n",
    "        continue\n",
    "    tp = (s[0], float(s[1]), float(s[2][:-4]))\n",
    "    if tp not in hnitlisti:\n",
    "        hnitlisti.append(tp)\n",
    "\"\"\"\n",
    "print(len(hnitlisti))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9befe0c-77da-4c6c-a169-b550da856dad",
   "metadata": {},
   "source": [
    "Viรฐ getum ekki tekiรฐ myndir af miรฐjunni skjรกsins, รพvรญ รพaรฐ eru รถnnur HTML-efni sem hylja gervitunglamyndirnar. รess vegna leitum viรฐ aรฐ staรฐi sem eru 0.002ยฐ til austurs frรก myndatรถkustaรฐnum; hรฉrna eru hnitin รก skjรกnum fyrir รพennan staรฐ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027be46b-66c6-413c-b70c-e59611deffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skhn = (746, 861)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7197e-1815-456d-bfd1-fdd04936ba0b",
   "metadata": {},
   "source": [
    "Viรฐ tรถkum skjรกmyndir af รถllum stรถรฐum รก hnitlistanum.\n",
    "- URL-iรฐ notar ISN93-hnit, en okkar hnit eru WGS84, og รพaรฐ er engin einfรถld leiรฐ til aรฐ skipta milli รพeirra. รess vegna รพurfum viรฐ aรฐ leita aรฐ hnitum eins og manneskja myndi leita.\n",
    "- ร Chrome er myndasvรฆรฐiรฐ 512x512, en รพegar myndin er vistuรฐ verรฐur hรบn 1024x1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b85159f-82ed-4569-b14e-8ce2a6794e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04561805725097656\n"
     ]
    }
   ],
   "source": [
    "byrjun = time.time()\n",
    "f = False\n",
    "for n in range(len(hnitlisti)):\n",
    "    hnit = hnitlisti[n]\n",
    "    try:\n",
    "        z = open(mappa + hnit[0] + '_' + str(hnit[1]) + '_' + str(hnit[2]) + '.png', 'rb')\n",
    "        z.close()\n",
    "    except FileNotFoundError:\n",
    "        f = True\n",
    "        break\n",
    "if f:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.set_window_size(1500, 1000)\n",
    "    driver.get('https://ja.is/kort/?x=356954&y=408253&nz=17.00&type=aerialnl')\n",
    "    # Accept GDPR\n",
    "    try:\n",
    "        btn = driver.find_element(By.XPATH, '//a[@id=\"gdpr_banner_ok\"]')\n",
    "        btn.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    # Allow cookies\n",
    "    try:\n",
    "        btn = driver.find_element(By.XPATH, '//button[@class=\"ch2-btn ch2-allow-all-btn ch2-btn-primary\"]')\n",
    "        btn.click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    leit = driver.find_element(By.XPATH, '//input[@id=\"mapq\"]')\n",
    "    for n in range(len(hnitlisti)):\n",
    "        if n % 100 == 0:\n",
    "            print(n, time.time() - byrjun)\n",
    "        hnit = hnitlisti[n]\n",
    "        try:\n",
    "            # Does file exist?\n",
    "            z = open(mappa + hnit[0] + '_' + str(hnit[1]) + '_' + str(hnit[2]) + '.png', 'rb')\n",
    "            z.close()\n",
    "        except FileNotFoundError:\n",
    "            # Input search term into search box\n",
    "            leit.clear()\n",
    "            leit.send_keys(str(hnit[1]) + ', ' + str(hnit[2] + 0.002))\n",
    "            leit.send_keys(Keys.RETURN)\n",
    "            time.sleep(2) # Wait for images to load\n",
    "            try:  # Place not found\n",
    "                nf = driver.find_element(By.XPATH, '//div[@class=\"row not-found\"]')\n",
    "            except NoSuchElementException:  # Place found, save and crop screenshot\n",
    "                driver.save_screenshot(mappa + hnit[0] + '_' + str(hnit[1]) + '_' + str(hnit[2]) + '.png')\n",
    "                skmynd = Image.open(mappa + hnit[0] + '_' + str(hnit[1]) + '_' + str(hnit[2]) + '.png')\n",
    "                skmynd = skmynd.crop((skhn[0] - 512, skhn[1] - 512, skhn[0] + 512, skhn[1] + 512))\n",
    "                skmynd.save(mappa + hnit[0] + '_' + str(hnit[1]) + '_' + str(hnit[2]) + '.png')\n",
    "            time.sleep(1)\n",
    "    driver.close()\n",
    "print(time.time() - byrjun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4be53-241f-47e9-8bf5-61fbb7134c93",
   "metadata": {},
   "source": [
    "## Staรฐsetningar vega\n",
    "Viรฐ sรฆkjum gรถgn frรก OpenStreetMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a890d29-f591-4459-8221-24fe5601231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = get_data('Iceland')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec4b6f-00f5-4b0c-a66b-1940697b3d2a",
   "metadata": {},
   "source": [
    "รetta undirforrit tekur dรญl รก myndinni og finnur GPS-hnitin. Viรฐ notum Web Mercator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0536b8-b22f-41ed-9372-7df9a9cfb69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pix2coord(pix, hnit_br):\n",
    "    x_t = hnit_br[1] + (pix[0] - 512) / 1024\n",
    "    lon = math.degrees(x_t * 2 * math.pi / (2 ** 17) - math.pi)\n",
    "    y_t = hnit_br[0] + (pix[1] - 512) / 1024\n",
    "    lat = math.degrees(2 * (math.atan(math.exp(math.pi - y_t * 2 * math.pi / (2 ** 17))) - math.pi / 4))\n",
    "    return (lat, lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a675895-2250-4bd4-b5fd-56386ec8da38",
   "metadata": {},
   "source": [
    "รetta undirforrit tekur GPS-hnit og finnur dรญlinn รก myndinni. Forritaรฐ meรฐ aรฐstoรฐ frรก o1-preview eftir OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35165ce-22f9-4562-a902-22e3249acf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2pix(lat, lon, hnit_br):\n",
    "    # Convert degrees to radians\n",
    "    lon_radians = math.radians(lon)\n",
    "    lat_radians = math.radians(lat)\n",
    "    \n",
    "    # Invert the calculation for x_t\n",
    "    x_t = ((lon_radians + math.pi) * (2 ** 17)) / (2 * math.pi)\n",
    "    # Calculate pix[0] (x-coordinate)\n",
    "    pix_x = (x_t - hnit_br[1]) * 1024 + 512\n",
    "    \n",
    "    # Invert the calculation for y_t\n",
    "    b = lat_radians / 2 + math.pi / 4\n",
    "    a = math.tan(b)\n",
    "    c = math.pi - math.log(a)\n",
    "    y_t = c * (2 ** 17) / (2 * math.pi)\n",
    "    # Calculate pix[1] (y-coordinate)\n",
    "    pix_y = (y_t - hnit_br[0]) * 1024 + 512\n",
    "    \n",
    "    return (pix_x, pix_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eecab4-b5e7-4950-ba40-4764bfe3810e",
   "metadata": {},
   "source": [
    "Viรฐ sรฆkjum tvo lista: einn yfir vegi รก Hรถfuรฐborgarsvรฆรฐinu, og einn รก Akureyri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0530e148-868a-4de1-8204-d2e3dbd7462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/networks.py:37: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  edges, nodes = prepare_geodataframe(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.75565218925476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyrosm/networks.py:37: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  edges, nodes = prepare_geodataframe(\n"
     ]
    }
   ],
   "source": [
    "byrjun = time.time()\n",
    "veg_listi = {}\n",
    "osm_h = OSM(fp, bounding_box=[-22.140901, 63.847886, -21.152576, 64.390306])\n",
    "veg_listi['h'] = osm_h.get_network(network_type='driving')\n",
    "osm_a = OSM(fp, bounding_box=[-18.398071, 65.543087, -17.968359, 66.576398])\n",
    "veg_listi['a'] = osm_a.get_network(network_type='driving')\n",
    "print(time.time() - byrjun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c32165-714b-4af9-bfb0-dab633f7954b",
   "metadata": {},
   "source": [
    "Viรฐ bรบum til lista yfir vegamรณt รก hverri mynd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa351946-9653-4f41-8013-8a4b53ff9074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.2554597854614258\n",
      "100 21 8.343374967575073\n",
      "200 37 14.935734987258911\n",
      "300 67 25.23120093345642\n",
      "400 90 33.767056941986084\n",
      "500 115 42.50995492935181\n",
      "600 134 49.70184874534607\n",
      "700 164 59.20858693122864\n",
      "800 189 67.62689805030823\n",
      "900 223 77.99738907814026\n",
      "1000 246 86.0568687915802\n",
      "1100 259 92.10099792480469\n",
      "1200 267 96.56886887550354\n",
      "1300 285 103.67087888717651\n",
      "1400 323 115.31606793403625\n",
      "1500 353 125.15462899208069\n",
      "1600 390 136.0856330394745\n",
      "1700 396 139.9398410320282\n",
      "143.81369996070862\n"
     ]
    }
   ],
   "source": [
    "byrjun = time.time()\n",
    "X_gogn = []\n",
    "y_gogn = []\n",
    "bd_all = {}\n",
    "for st in ['h', 'a']:\n",
    "    vegir = veg_listi[st]\n",
    "    bd = []\n",
    "    for k in range(vegir.shape[0]):\n",
    "        bns = vegir['geometry'][k].bounds\n",
    "        bd.append(bns)\n",
    "    bd_all[st] = bd\n",
    "for n in range(len(hnitlisti)):\n",
    "    if n % 100 == 0:\n",
    "        print(n, len(X_gogn), time.time() - byrjun)\n",
    "    hnit = hnitlisti[n]\n",
    "    \n",
    "    # Open image\n",
    "    try:\n",
    "        gtm = Image.open(mappa + hnit[0] + '_' + str(hnit[1]) + '_' + str(hnit[2]) + '.png')\n",
    "        dilar = gtm.load()\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    except OSError:\n",
    "        print('OSError', hnit)\n",
    "        continue\n",
    "        \n",
    "    # Convert coordinates\n",
    "    y_n = 1 / (2 * math.pi) * 2 ** 17 * (math.pi - math.log(math.tan(math.pi / 4 + math.radians(hnit[1]) / 2))) - 0.5\n",
    "    y_s = 1 / (2 * math.pi) * 2 ** 17 * (math.pi - math.log(math.tan(math.pi / 4 + math.radians(hnit[1]) / 2))) + 0.5\n",
    "    if hnit[0] == 'd':\n",
    "        vegir = veg_listi['h']\n",
    "    else:\n",
    "        vegir = veg_listi[hnit[0]]\n",
    "    hnit_br = (1 / (2 * math.pi) * 2 ** 17 * (math.pi - math.log(math.tan(math.pi / 4 + math.radians(hnit[1]) / 2))),\n",
    "               1 / (2 * math.pi) * 2 ** 17 * (math.pi + math.radians(hnit[2])))\n",
    "    \n",
    "    # Roads\n",
    "    y_mynd = np.zeros((1, 1, 1024, 1024))\n",
    "    if hnit[0] == 'd':\n",
    "        bd = bd_all['h']\n",
    "    else:\n",
    "        bd = bd_all[hnit[0]]\n",
    "    hnitskra = {}\n",
    "    for v in range(vegir.shape[0]):\n",
    "        bns = bd[v]\n",
    "        if bns[0] > hnit[2] + 0.0014 or bns[1] > hnit[1] + 0.0008 or bns[2] < hnit[2] - 0.0014 or bns[3] < hnit[1] - 0.0008:\n",
    "            continue\n",
    "        NW = coord2pix(bns[3], bns[0], hnit_br)\n",
    "        SE = coord2pix(bns[1], bns[2], hnit_br)\n",
    "        if NW[0] > 1024 or NW[1] > 1024 or SE[0] < 0 or SE[1] < 0:\n",
    "            continue\n",
    "        for p in vegir.loc[v, 'geometry'].geoms:\n",
    "            x, y = p.xy\n",
    "            try:\n",
    "                hnitskra[(y[0], x[0])] += 1\n",
    "            except KeyError:\n",
    "                hnitskra[(y[0], x[0])] = 1\n",
    "            try:\n",
    "                hnitskra[(y[1], x[1])] += 1\n",
    "            except KeyError:\n",
    "                hnitskra[(y[1], x[1])] = 1\n",
    "    mot = []\n",
    "    for p in hnitskra:\n",
    "        if hnitskra[p] > 2:\n",
    "            ct = coord2pix(p[0], p[1], hnit_br)\n",
    "            if ct[0] >= 0 and ct[0] < 1024 and ct[1] >= 0 and ct[1] < 1024:\n",
    "                mot.append(ct)\n",
    "    if hnit[0] != 'd' and len(mot) > 0:\n",
    "        X_gogn.append(torch.tensor(np.transpose(np.array(gtm.getdata()).reshape(1024, 1024, 3), \n",
    "                                                (2, 0, 1)).reshape(1, 3, 1024, 1024), dtype=torch.float32).to(device))\n",
    "        y_gogn.append(torch.tensor(np.array(mot).reshape(1, -1, 2), dtype=torch.float32).to(device))\n",
    "    gtm.close()\n",
    "print(time.time() - byrjun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33e79e8-a454-43dc-b4ef-3ae3433ed6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_gogn, y_gogn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1c583-8900-4bd8-8a8c-ffe2817b22e4",
   "metadata": {},
   "source": [
    "## Lรญkan 1\n",
    "Hรฉrna er CNN sem bรฝr til tvรฆr greypingar: eina fyrir myndina og eina fyrir hnitlistann. Markmiรฐ รพessa lรญkans er aรฐ greypingar sรฉu samar.\n",
    "\n",
    "Forritaรฐ meรฐ aรฐstoรฐ frรก o1-preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee8f219-7df7-42c3-8bb2-acf494c9cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_points_embedding = nn.Parameter(torch.randn(128).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c26b9c6-7105-41bb-9e36-a26fd7c8133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CNN for image processing (ResNet18 as an example)\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Using a pre-trained ResNet18 model\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        # Modify the last layer to output a 128-dimensional feature vector\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.resnet(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff1e48e-bdc0-4de2-b299-0f4862de3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM-based embedding model for coordinates\n",
    "class CoordinateEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(CoordinateEmbedding, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, coordinates):\n",
    "        lstm_out, _ = self.lstm(coordinates)\n",
    "        embedding = self.fc(lstm_out[:, -1, :])  # Use last hidden state as embedding\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8202373c-2123-4d7d-bed2-f5eb513eb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both models into a final training model\n",
    "class SatelliteModel(nn.Module):\n",
    "    def __init__(self, image_encoder, coordinate_encoder, embedding_size=128):\n",
    "        super(SatelliteModel, self).__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.coordinate_encoder = coordinate_encoder\n",
    "        # Define a special embedding for \"no points\" (either zero or learnable)\n",
    "        self.no_points_embedding = no_points_embedding\n",
    "\n",
    "    def forward(self, images, coordinates=None):\n",
    "        # Encode the image\n",
    "        image_features = self.image_encoder(images)  # Shape: (batch_size, 128)\n",
    "        \n",
    "        if coordinates is not None and coordinates.shape[1] > 0:\n",
    "            # If coordinates exist, get the coordinate embedding\n",
    "            coordinate_embedding = self.coordinate_encoder(coordinates)\n",
    "        else:\n",
    "            # If no coordinates, use the \"no points\" embedding\n",
    "            coordinate_embedding = self.no_points_embedding.expand(images.size(0), -1)\n",
    "\n",
    "        return image_features, coordinate_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb89dbc-5300-4da1-a486-fbf879efa156",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76f84000-11c7-472b-a90f-9ff339a42297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "byrjun = time.time()\n",
    "# Initialize both models\n",
    "image_encoder = ImageEncoder()\n",
    "coordinate_encoder = CoordinateEmbedding(input_size=2, hidden_size=128, output_size=128)\n",
    "\n",
    "# Combine into final model\n",
    "likan = SatelliteModel(image_encoder, coordinate_encoder).to(device)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Since we are predicting embeddings, we can use MSELoss\n",
    "optimizer = optim.Adam(likan.parameters(), lr=1e-4)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    likan.eval()\n",
    "    with torch.no_grad():\n",
    "        rl = 0.0\n",
    "        for i in range(len(X_train)):\n",
    "            image_features, coordinate_embedding = likan(X_train[i], y_train[i])\n",
    "            loss = criterion(image_features, coordinate_embedding)\n",
    "            rl += loss.item()\n",
    "        train_loss.append(rl / len(X_train))\n",
    "        rl = 0.0\n",
    "        for i in range(len(X_test)):\n",
    "            image_features, coordinate_embedding = likan(X_test[i], y_test[i])\n",
    "            loss = criterion(image_features, coordinate_embedding)\n",
    "            rl += loss.item()\n",
    "        test_loss.append(rl / len(X_test))\n",
    "    torch.mps.empty_cache()\n",
    "    \n",
    "    likan.train()\n",
    "    for i in range(len(X_train)):\n",
    "    # Forward pass: get image features and coordinate embeddings\n",
    "        image_features, coordinate_embedding = likan(X_train[i], y_train[i])\n",
    "        \n",
    "        # Loss: comparing image features with coordinate embeddings\n",
    "        loss = criterion(image_features, coordinate_embedding)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    # Print loss for the current epoch\n",
    "    print(f'Epoch [{e+1}/{epochs}], Time: {time.time() - byrjun}')\n",
    "\n",
    "likan.eval()\n",
    "with torch.no_grad():\n",
    "    rl = 0.0\n",
    "    for i in range(len(X_train)):\n",
    "        image_features, coordinate_embedding = likan(X_train[i], y_train[i])\n",
    "        loss = criterion(image_features, coordinate_embedding)\n",
    "        rl += loss.item()\n",
    "    train_loss.append(rl / len(X_train))\n",
    "    rl = 0.0\n",
    "    for i in range(len(X_test)):\n",
    "        image_features, coordinate_embedding = likan(X_test[i], y_test[i])\n",
    "        loss = criterion(image_features, coordinate_embedding)\n",
    "        rl += loss.item()\n",
    "    test_loss.append(rl / len(X_test))\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c09b2-0562-4fee-85d5-5e1b810ab1fe",
   "metadata": {},
   "source": [
    "### Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b3986-63d1-4a21-857b-9e16943b48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train loss:', train_loss[-1])\n",
    "print('Test loss:', test_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba751b-b634-4ad5-8c6c-ac64733a80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(test_loss, label='Test')\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c4acb-6a0c-4934-88c3-b51cfd46e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss[1:], label='Train')\n",
    "plt.plot(test_loss[1:], label='Test')\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05fd19-85ff-43c4-9066-cda064781402",
   "metadata": {},
   "source": [
    "## Lรญkan 2\n",
    "รetta lรญkan tekur hnitlistagreypingu og reynir aรฐ รบtreikna hnitlistann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad05c2-b94a-447e-9197-2b5d5f054cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_distance(pred_points, target_points):\n",
    "    \"\"\"\n",
    "    pred_points: List of tensors of shape (Ni, D), where Ni is the number of points in the i-th prediction.\n",
    "    target_points: List of tensors of shape (Mi, D), where Mi is the number of points in the i-th target.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    batch_size = len(pred_points)\n",
    "    for pred, target in zip(pred_points, target_points):\n",
    "        if pred.numel() == 0 and target.numel() == 0:\n",
    "            # Both sequences are empty; loss is zero.\n",
    "            loss = 0.0\n",
    "        elif pred.numel() == 0 or target.numel() == 0:\n",
    "            # One sequence is empty; define loss appropriately (e.g., sum of squared norms).\n",
    "            non_empty = pred if pred.numel() != 0 else target\n",
    "            loss = non_empty.pow(2).sum()\n",
    "        else:\n",
    "            # Compute pairwise distances.\n",
    "            diff = pred.unsqueeze(1) - target.unsqueeze(0)  # Shape: (Ni, Mi, D)\n",
    "            dist = torch.norm(diff, dim=2)  # Shape: (Ni, Mi)\n",
    "            # For each point in pred, find the closest point in target.\n",
    "            min_pred_to_target = dist.min(dim=1)[0]\n",
    "            # For each point in target, find the closest point in pred.\n",
    "            min_target_to_pred = dist.min(dim=0)[0]\n",
    "            # Sum the minimal distances.\n",
    "            loss = min_pred_to_target.sum() + min_target_to_pred.sum()\n",
    "        total_loss += loss\n",
    "    return total_loss / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bbeab-4cac-430a-9faa-85243e850acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateDecoder(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size=2, num_layers=1):\n",
    "        super(CoordinateDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # The initial hidden state is derived from the embedding\n",
    "        self.fc_embed = nn.Linear(embedding_size, hidden_size * num_layers)\n",
    "\n",
    "        # LSTM decoder\n",
    "        self.lstm = nn.LSTM(input_size=output_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.fc_stop = nn.Linear(hidden_size, 1)  # Predicts probability to stop\n",
    "\n",
    "    def forward(self, embedding, max_seq_length):\n",
    "        batch_size = embedding.size(0)\n",
    "\n",
    "        # Initialize hidden state from embedding\n",
    "        h_0 = self.fc_embed(embedding)\n",
    "        h_0 = h_0.view(self.num_layers, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros_like(h_0)  # You can also learn c_0 or set it differently\n",
    "\n",
    "        # Prepare the initial input (e.g., start token or zeros)\n",
    "        decoder_input = torch.zeros(batch_size, 1, 2).to(embedding.device)  # Starting with zero coordinates\n",
    "\n",
    "        outputs = []\n",
    "        stop_probs = []\n",
    "        for t in range(max_seq_length):\n",
    "            # LSTM step\n",
    "            output, (h_0, c_0) = self.lstm(decoder_input, (h_0, c_0))\n",
    "            coord_pred = self.fc_out(output)\n",
    "            stop_logit = self.fc_stop(output)\n",
    "            stop_prob = torch.sigmoid(stop_logit)\n",
    "\n",
    "            outputs.append(coord_pred)\n",
    "            stop_probs.append(stop_prob)\n",
    "\n",
    "            # Check if all sequences have predicted stop\n",
    "            if torch.all(stop_prob > 0.5):\n",
    "                break\n",
    "\n",
    "            decoder_input = coord_pred  # Or use teacher forcing\n",
    "            \n",
    "        if len(outputs) == 0:\n",
    "            outputs = torch.tensor(np.zeros((1, 0, 2)), dtype=torch.float32).to(device)\n",
    "            stop_probs = torch.tensor(np.zeros((0, 1)), dtype=torch.float32).to(device)\n",
    "            return outputs, stop_probs\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        stop_probs = torch.cat(stop_probs, dim=1)\n",
    "        return outputs, stop_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafe22a-1754-4f39-bbd8-c47b0e957908",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max([z.shape[1] for z in y_train])\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72ae1f-cca7-4932-a9a8-7ca34c0a6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_gogn)):\n",
    "    X_gogn[i] = None\n",
    "del X_gogn\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = None\n",
    "del X_train\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = None\n",
    "del X_test\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14661b8c-e8d6-4570-8732-6166a9fc6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "byrjun = time.time()\n",
    "# Initialize the decoder\n",
    "coordinate_decoder = CoordinateDecoder(embedding_size=128, hidden_size=128).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "decoder_optimizer = optim.Adam(coordinate_decoder.parameters(), lr=1e-4)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# DEBUG\n",
    "minni_a = []\n",
    "minni_u = []\n",
    "minni_m = []\n",
    "# END DEBUG\n",
    "\n",
    "# Training loop for the decoder\n",
    "likan.eval()\n",
    "for e in range(epochs):\n",
    "    coordinate_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        rl = 0.0\n",
    "        for i in range(len(y_train)):\n",
    "            coord_embeddings = likan.coordinate_encoder(y_train[i])\n",
    "            seq_length = y_train[i].size(1)\n",
    "            output_sequences, stop_probs = coordinate_decoder(coord_embeddings, seq_length)\n",
    "            loss = chamfer_distance(output_sequences, y_train[i])\n",
    "            rl += loss.item()\n",
    "        train_loss.append(rl / len(y_train))\n",
    "        rl = 0.0\n",
    "        for i in range(len(y_test)):\n",
    "            coord_embeddings = likan.coordinate_encoder(y_test[i])\n",
    "            seq_length = y_train[i].size(1)\n",
    "            output_sequences, stop_probs = coordinate_decoder(coord_embeddings, seq_length)\n",
    "            loss = chamfer_distance(output_sequences, y_test[i])\n",
    "            rl += loss.item()\n",
    "        test_loss.append(rl / len(y_test))\n",
    "\n",
    "    torch.mps.empty_cache()\n",
    "        \n",
    "    coordinate_decoder.train()\n",
    "    for i in range(len(y_train)):\n",
    "        # Get image embeddings\n",
    "        with torch.no_grad():\n",
    "            coord_embeddings = likan.coordinate_encoder(y_train[i])\n",
    "\n",
    "        # Forward pass thru decoder\n",
    "        seq_length = y_train[i].size(1)\n",
    "        output_sequences, stop_probs = coordinate_decoder(coord_embeddings, seq_length)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = chamfer_distance(output_sequences, y_train[i])\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        decoder_optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    minni_m.append(torch.mps.current_allocated_memory())  # DEBUG\n",
    "    minni_a.append(psutil.virtual_memory().available)  # DEBUG\n",
    "    minni_u.append(psutil.virtual_memory().used)  # DEBUG\n",
    "\n",
    "    print(f'Epoch [{e+1}/{epochs}], Time: {time.time() - byrjun}')\n",
    "\n",
    "coordinate_decoder.eval()\n",
    "with torch.no_grad():\n",
    "    rl = 0.0\n",
    "    for i in range(len(y_train)):\n",
    "        coord_embeddings = likan.coordinate_encoder(y_train[i])\n",
    "        seq_length = y_train[i].size(1)\n",
    "        output_sequences, stop_probs = coordinate_decoder(coord_embeddings, seq_length)\n",
    "        loss = chamfer_distance(output_sequences, y_train[i])\n",
    "        rl += loss.item()\n",
    "    train_loss.append(rl / len(y_train))\n",
    "    rl = 0.0\n",
    "    for i in range(len(y_test)):\n",
    "        coord_embeddings = likan.coordinate_encoder(y_test[i])\n",
    "        seq_length = y_train[i].size(1)\n",
    "        output_sequences, stop_probs = coordinate_decoder(coord_embeddings, seq_length)\n",
    "        loss = chamfer_distance(output_sequences, y_test[i])\n",
    "        rl += loss.item()\n",
    "    test_loss.append(rl / len(y_test))\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d997b4-89b6-41fc-8fd3-196f3a998816",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minni_a[-1], minni_u[-1], minni_m[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11714dc3-ec36-46e7-9be9-1dfc29f2da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(minni_a, label='Available')\n",
    "plt.plot(minni_u, label='Used')\n",
    "plt.title('Memory from psutil')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Memory')\n",
    "plt.ylim(bottom=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23511f59-d61e-4b2f-be06-dbd41b2d12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(minni_m)\n",
    "plt.title('MPS memory from PyTorch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Memory')\n",
    "plt.ylim(bottom=0, top=2e9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462274e-15f8-4750-a70e-2f6ece350e3a",
   "metadata": {},
   "source": [
    "### Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb594fb3-2120-4125-9a88-664251849269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train loss:', train_loss[-1])\n",
    "print('Test loss:', test_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1717f-a105-4e95-a6d1-8bf50a6f11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(test_loss, label='Test')\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f920db7-46af-4a49-9ccd-9d4e887e41ef",
   "metadata": {},
   "source": [
    "## Sรฝning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02648ef2-dafe-49c3-8f7a-804fa911a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "likan.eval()\n",
    "coordinate_decoder.eval()\n",
    "profmynd = Image.open(mappa + 'd_64.150678_-21.946834.png')\n",
    "X_mynd = torch.tensor(np.transpose(np.array(profmynd.getdata()).reshape(1024, 1024, 3), \n",
    "                                   (2, 0, 1)).reshape(1, 3, 1024, 1024), dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get image embeddings\n",
    "    image_embeddings = likan.image_encoder(X_mynd)\n",
    "\n",
    "    # Decide on the maximum sequence length you want to generate\n",
    "    max_seq_length = 20  # Adjust as needed\n",
    "\n",
    "    # Generate coordinate sequences\n",
    "    generated_sequences, prob = coordinate_decoder(image_embeddings, max_seq_length)\n",
    "\n",
    "    # Now 'generated_sequences' contains the predicted coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e28d68-2474-462a-9d17-fb1f9da98b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e8d55-844f-4daf-8842-3284d8421836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'generated_sequences' is of shape (batch_size, seq_length, 2)\n",
    "coordinates = generated_sequences[0].cpu().numpy()  # Get coordinates for the first image\n",
    "\n",
    "# Plot the coordinates\n",
    "plt.imshow(profmynd)\n",
    "plt.title('Predicted')\n",
    "plt.scatter(coordinates[:, 1], coordinates[:, 0], marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532eee4-7f2b-43b8-ae3b-f75f05ba8560",
   "metadata": {},
   "source": [
    "## Lokaorรฐ\n",
    "Greiningin er enn รญ vinnslu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
